# ğŸš€ Fake News Detection on WELFake Dataset
### TF-IDF + Meta Features + LightGBM (12GB RAM Optimized)

> A high-performance, memory-efficient Fake News Detection pipeline built using Sparse TF-IDF and LightGBM.  
> Designed to run safely on Google Colab (12GB RAM).

---

## ğŸ§  Overview

This project builds a robust binary classifier to detect **Fake vs Real News** using:

- Advanced text preprocessing (lemmatization + optimized stopwords)
- Sparse TF-IDF (1â€“2 grams)
- Custom handcrafted meta features
- LightGBM with early stopping
- Memory-optimized architecture

The system is built to handle large-scale text data efficiently without exceeding Colab memory limits.

---

## ğŸ—ï¸ Model Architecture

```
Raw Text
   â†“
Text Cleaning + Lemmatization
   â†“
TF-IDF (25K Sparse Features)
   +
Meta Features (4 Linguistic Signals)
   â†“
Feature Concatenation (Sparse Matrix)
   â†“
LightGBM Classifier
   â†“
Prediction
```

---

## ğŸ“‚ Dataset

- **Training File:** `WELFake_Dataset.csv`
- **Test File:** `test.csv`

### Target Labels:
- `0` â†’ Real News  
- `1` â†’ Fake News  

---

## âš™ï¸ Feature Engineering

### ğŸ”¤ Text Features
- TF-IDF Vectorizer
- Max Features: 25,000
- N-grams: (1,2)
- min_df = 5
- max_df = 0.9
- Sublinear TF scaling
- English stopword removal
- WordNet Lemmatization
- Preserved negation words: *no, not, never*

### ğŸ“Š Meta Features

Additional handcrafted linguistic features:

- `char_len` â†’ Total character length
- `word_len` â†’ Total word count
- `caps_ratio` â†’ Ratio of uppercase characters
- `punct_count` â†’ Count of punctuation (!?.)

These features capture stylistic patterns often seen in misinformation content.

---

## ğŸŒ³ LightGBM Configuration

| Parameter | Value |
|------------|--------|
| n_estimators | 3000 |
| learning_rate | 0.03 |
| num_leaves | 64 |
| subsample | 0.8 |
| colsample_bytree | 0.8 |
| class_weight | balanced |
| early_stopping | 100 rounds |

---

## ğŸ§ª Training Strategy

- Train/Validation Split: **85% / 15%**
- Stratified sampling
- Evaluation Metrics:
  - Accuracy
  - Classification Report
  - Binary Log Loss

---

## ğŸ’¾ Memory Optimization (12GB Safe)

- Sparse TF-IDF matrix
- Reduced feature space (25k)
- No cross-validation (single split)
- Explicit garbage collection
- Efficient LightGBM implementation

Built specifically to prevent Colab crashes.

---

## ğŸ› ï¸ Installation

```bash
pip install lightgbm nltk scikit-learn
```

---

## â–¶ï¸ How to Run

1. Upload the dataset files to Google Colab:
   - `WELFake_Dataset.csv`
   - `test.csv`

2. Run the script or notebook.

3. Output generated:

```
submission_final.csv
```

Ready for submission.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ WELFake_Dataset.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ fake_news_model.ipynb
â”œâ”€â”€ submission_final.csv
â””â”€â”€ README.md
```

---

## ğŸ“Š Output

The final output file:

```
submission_final.csv
```

Contains:

| id | label |
|----|-------|
| Article ID | Predicted Class (0 or 1) |

---

## ğŸš€ Why This Approach?

- Handles large datasets efficiently
- Combines lexical + stylistic signals
- Fast training with strong performance
- Production-ready scalable pipeline
- Clean and interpretable architecture

---

## ğŸ”® Future Improvements

- Add cross-validation
- Hyperparameter tuning with Optuna
- Add sentiment polarity features
- Experiment with transformer embeddings
- Model ensembling

---

## ğŸ“œ License

This project is open-source and available under the MIT License.

---

â­ If you found this useful, consider giving the repository a star!
